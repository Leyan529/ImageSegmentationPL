# ImageSegmentation
My Frame work for Image Semantic Segmentation with pytorch Lightning + Albumentations
## Overview
I organizize the object detection algorithms proposed in recent years, and focused on **`Cityscapes`**, **`COCO`** , **`Pascal VOC`** and **`BDD100K`** Dataset.
This frame work also include **`EarlyStopping mechanism`**.

### Status
- [x] DataSet
- [x] Training/Validiation Step
- [x] TensorBoardLogger callback
- [x] ModelCheckpoint callback
- [x] Learning Rate Monitor
- [x] GPUStats Monitor
- [x] Early stop callback
- [x] Read Best model path
- [x] Lightning with Argument Parser
- [x] Yaml custom config

## Datasets:

I used 3 different datases: **`Cityscapes`, `COCO`, `Pascal VOC`** and **`BDD100K`**. Statistics of datasets I used for experiments is shown below

- **VOC**:
  Download the voc images and annotations from [VOC2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007) or [VOC2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012). Make sure to put the files as the following structure:

-- VOC2007
![](https://i.imgur.com/wncA2wC.png)

-- VOC2012
![](https://i.imgur.com/v3AQelB.png)

  
  
| Dataset                | Classes | #Train images/objects | #Validation images/objects |
|------------------------|:---------:|:-----------------------:|:----------------------------:|
| VOC2007                |    20   |      209/633       |          213/582        |
| VOC2012                |    20   |      1464/3507     |         1449/3422       |

  ```
  VOCDevkit
  ├── VOC2007
  │   ├── JPEGImages  
  │   ├── SegmentationClass
  │   ├── ...
  │   └── ...
  └── VOC2012
      ├── JPEGImages  
      ├── SegmentationClass
      ├── ...
      └── ...
  ```
  
- **COCO**:
  Download the coco images and annotations from [coco website](http://cocodataset.org/#download). Make sure to put the files as the following structure:

| Dataset                | Classes | #Train images/objects | #Validation images/objects |
|------------------------|:---------:|:-----------------------:|:----------------------------:|
| COCO2014               |    21   |         64k/-         |            31k/-           |
| COCO2017               |    21   |         92k/-        |             3k/-           |
```
  COCO
  ├── annotations
  │   ├── instances_train2014.json
  │   ├── instances_train2017.json
  │   ├── instances_val2014.json
  │   └── instances_val2017.json
  │── images
  │   ├── train2014
  │   ├── train2017
  │   ├── val2014
  │   └── val2017
  └── mask
      ├── train2014
      ├── train2017
      ├── val2014
      └── val2017
```

- **Cityscapes**:
The Cityscapes Dataset focuses on semantic understanding of urban street scenes. In the following, we give an overview on the design choices that were made to target the dataset’s focus.

![](https://i.imgur.com/Dgi4K9S.png)



  Download the container images and annotations from [cityscapes](https://www.cityscapes-dataset.com/downloads/). Make sure to put the files as the following structure:
 ![](https://i.imgur.com/rRJSIYQ.png)
![](https://i.imgur.com/L3bVJFM.png)  
```
  Cityscapes
  ├── leftImg8bit
  │   ├── train
  │   ├── val
  │   ├── test  
  │     
  │── gtFine_trainvaltest
      ├── gtFine
          ├── train
          ├── val
          ├── test 
```
- **BDD100K**:
BDD100K is a diverse driving dataset for heterogeneous multitask learning.
![image](https://user-images.githubusercontent.com/24097516/121802720-6ce0b000-cc70-11eb-85f9-a757bf946796.png)

Format
This is compatible with the labels generated by Scalabel. A label json file is a list of frame objects with the fields below. Please note that this format is a superset of the data fields. For example, box3d may be absent if the label is a 2d bounding box, and intrinsics may not appear if the exact camera calibration is unknown.

- name: string
- url: string
- videoName: string (optional)
- attributes:
    - weather: "rainy|snowy|clear|overcast|undefined|partly cloudy|foggy"
    - scene: "tunnel|residential|parking lot|undefined|city street|gas stations|highway|"
    - timeofday: "daytime|night|dawn/dusk|undefined"
- intrinsics
    - focal: [x, y]
    - center: [x, y]
    - nearClip:
- extrinsics
    - location
    - rotation
- timestamp: int64 (epoch time ms)
- frameIndex: int (optional, frame index in this video)
- labels [ ]:
    - id: int32
    - category: string (classification)
    - manualShape: boolean (whether the shape of the label is created or modified manually)
    - manualAttributes: boolean (whether the attribute of the label is created or modified manually)
    - score: float (the confidence or some other ways of measuring the quality of the label.)
    - attributes:
        - occluded: boolean
        - truncated: boolean
        - trafficLightColor: "red|green|yellow|none"
        - areaType: "direct | alternative" (for driving area)
        - laneDirection: "parallel|vertical" (for lanes)
        - laneStyle: "solid | dashed" (for lanes)
        - laneTypes: (for lanes)
    - box2d:
       - x1: float
       - y1: float
       - x2: float
       - y2: float
   - box3d:
       - alpha: (observation angle if there is a 2D view)
       - orientation: (3D orientation of the bounding box, used for 3D point cloud annotation)
       - location: (3D point, x, y, z, center of the box)
       - dimension: (3D point, height, width, length)
   - poly2d: an array of objects, with the structure
       - vertices: [][]float (list of 2-tuples [x, y])
       - types: string (each character corresponds to the type of the vertex with the same index in vertices. ‘L’ for vertex and ‘C’ for control point of a bezier curve.
       - closed: boolean (closed for polygon and otherwise for path)
Road object categories:

[
    "bike",
    "bus",
    "car",
    "motor",
    "person",
    "rider",
    "traffic light",
    "traffic sign",
    "train",
    "truck"
]

Download the container images and annotations from [BDD100K](https://bdd-data.berkeley.edu/portal.html#download). Make sure to put the files as the following structure:

```
  bdd100k
  ├── bdd100k
    ├── images
      ├── 10k
        ├── train
        ├── val
        ├── test
    ├── labels            
      ├── sem_seg    
        ├── bitmasks
          ├── train
          ├── val
        ├── colormaps
          ├── train
          ├── val
        ├── polygons
          ├── ins_seg_train.json
          ├── ins_seg_val.json             
```


## Semantic Segmentation Models - based on LightningModule
- **FPN**
- **FCN**
- **DeConvNet**
- **UNet**
- **SegNet**
- **PSPNet**
- **DeepLabV3**
- **DeepLabv3_plus**

## Prerequisites
* **Windows 10**
* **CUDA 10.2**
* **NVIDIA GPU 1660 + CuDNN v7.605**
* **python 3.6.9**
* **pytorch 1.81**
* **opencv-python 4.1.1.26**
* **numpy 1.19.5**
* **torchvision 0.9**
* **torchsummary 1.5.1**
* **Pillow 7.2.0**
* **dlib==19.21**
* **tensorflow-gpu 2.2.0**
* **tensorboard 2.5.0** 
* **pytorch-lightning 1.2.6**
* **albumentations 0.5.2**


## Usage
### 0. Prepare the dataset
* **Download custom dataset in the  `data_paths`.** 
* **And create custom dataset `custom_dataset.py` in the `dataset`.**

### Execute (Train/Val + Test)

#### Cityscape
```python
python run.py --use CityscapeModule --model DeepLabv3_plus
```
#### VOC
```python
python run.py --use VOCModule --model DeepLabv3_plus
```
#### COCO
```python
python run.py --use COCOModule --model DeepLabv3_plus
```
#### BDD100K
```python
python run.py --use BDD100KModule --model DeepLabv3_plus
```


## Reference
- U-Net :  https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_model.py
- SegNet : https://github.com/Charmve/Semantic-Segmentation-PyTorch/blob/7095051118c79c322df8b98ab4e59caeb61c57a0/models/seg_net.py
- DeconvNet:   https://github.com/renkexinmay/SemanticSegmentation-FCN-DeconvNet/blob/master/model.py
- FCN: https://github.com/pochih/FCN-pytorch/blob/master/python/fcn.py
- PSPNet: https://github.com/Lextal/pspnet-pytorch/blob/4eb6ab61287e837f5e2d8c1ae09fadeaa0e31e37/pspnet.py
- DeepLabV3: https://github.com/fregu856/deeplabv3/blob/master/model/deeplabv3.py
- DeepLabv3_plus : https://github.com/MLearing/Pytorch-DeepLab-v3-plus/blob/master/networks/deeplab_resnet.py
- Dataset Preprare: https://github.com/jfzhang95/pytorch-deeplab-xception/tree/master/dataloaders/datasets
https://towardsdatascience.com/master-the-coco-dataset-for-semantic-image-segmentation-part-1-of-2-732712631047
- Frame & FPN : https://github.com/Andy-zhujunwen/FPN-Semantic-segmentation
- cityscapes: https://www.cityscapes-dataset.com/dataset-overview/#features
- VOC2007: https://pjreddie.com/media/files/VOC2007_doc.pdf
- VOC2012: https://pjreddie.com/media/files/VOC2012_doc.pdf
- Albumentations : https://albumentations.ai/docs/getting_started/mask_augmentation/
- pytorch-lightning : https://pytorch-lightning.readthedocs.io/en/latest/
- bdd100k colors: https://github.com/blue-oil/blueoil/blob/0128677dcdca0e6c87b550985a0cdebc681cc56f/blueoil/datasets/bdd100k.py#L162
